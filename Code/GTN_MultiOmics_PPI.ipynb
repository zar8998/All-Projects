{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q torch_geometric\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import TransformerConv\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import datetime\n",
        "\n",
        "now = datetime.datetime.now\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 1: Load the PPI data from GitHub\n",
        "ppi_url = 'https://raw.githubusercontent.com/zar8998/ppi/main/PPI.csv'\n",
        "ppi_df = pd.read_csv(ppi_url)\n",
        "\n",
        "# Step 2: Concatenate 'stringId_A' and 'stringId_B' to calculate the number of connections (degree)\n",
        "all_proteins = pd.concat([ppi_df['stringId_A'], ppi_df['stringId_B']])\n",
        "\n",
        "# Step 3: Count the number of connections for each protein\n",
        "protein_connections = all_proteins.value_counts()\n",
        "\n",
        "# Step 4: Define a degree threshold to select only highly connected proteins (e.g., 200 or more connections)\n",
        "degree_threshold = 200\n",
        "high_degree_proteins = protein_connections[protein_connections >= degree_threshold].index\n",
        "\n",
        "# Step 5: Filter the PPI data to include only edges where both proteins have a high number of connections\n",
        "ppi_filtered = ppi_df[\n",
        "    ppi_df['stringId_A'].isin(high_degree_proteins) &\n",
        "    ppi_df['stringId_B'].isin(high_degree_proteins)\n",
        "]\n",
        "\n",
        "# Step 6: Map the high-degree proteins to unique node IDs\n",
        "proteins = pd.concat([ppi_filtered['stringId_A'], ppi_filtered['stringId_B']]).unique()\n",
        "protein_to_id = {protein: idx for idx, protein in enumerate(proteins)}\n",
        "\n",
        "# Step 7: Create edge index (this will be the input for GTN)\n",
        "# Use apply to transform protein IDs into node IDs by accessing each protein in the row\n",
        "edges = ppi_filtered[['stringId_A', 'stringId_B']].apply(lambda row: [protein_to_id[row['stringId_A']], protein_to_id[row['stringId_B']]], axis=1)\n",
        "edges = np.array(edges.tolist()).T  # Convert the result to numpy array and transpose to get correct shape\n",
        "\n",
        "# Create edge_index tensor for PyTorch Geometric\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "# Step 8: Load and preprocess the multi-omics data\n",
        "!wget https://www.idahofallshighered.org/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n",
        "file_path = 'mRNA_miRNA_Meth_integrated.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "Y = df.iloc[:, -1].copy()\n",
        "\n",
        "# Remove non-numeric columns\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "X = df.values\n",
        "\n",
        "num_classes = len(set(Y))\n",
        "print(\"Number of classes:\", num_classes)\n",
        "num_samples = X.shape[0]\n",
        "print(\"Number of samples:\", num_samples)\n",
        "num_Features = X.shape[1]\n",
        "print(\"Number of Features:\", num_Features)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "\n",
        "# Convert data to PyTorch tensors and move to device\n",
        "X = torch.tensor(X, dtype=torch.float).to(device)\n",
        "Y = torch.tensor(Y, dtype=torch.long).to(device)\n",
        "\n",
        "# Step 9: Create PyTorch Geometric data object using the edge_index from the filtered PPI network\n",
        "data = Data(x=X, edge_index=edge_index)\n",
        "\n",
        "# Step 10: Define the GTN model\n",
        "class GTN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, hidden_feats=1024, num_layers=2, dropout=0.5):\n",
        "        super(GTN, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(num_features, hidden_feats, heads=1, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(TransformerConv(hidden_feats, hidden_feats, heads=1, dropout=dropout))\n",
        "        self.fc = nn.Linear(hidden_feats, num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 11: Set up K-fold cross-validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Initialize lists to store metrics for each fold\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "accuracy_scores = []\n",
        "F1Measure = []\n",
        "\n",
        "# Set hyperparameters\n",
        "hidden_feats = 1024\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "lr = 0.001\n",
        "weight_decay = 0\n",
        "num_epochs = 100\n",
        "\n",
        "# Step 12: Training and Evaluation\n",
        "t = now()\n",
        "for train_index, test_index in kf.split(X.cpu()):\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    # Create train/test data using the same PPI edge_index\n",
        "    train_data = Data(x=X_train, edge_index=edge_index)\n",
        "    test_data = Data(x=X_test, edge_index=edge_index)\n",
        "\n",
        "    # Create the GTN model\n",
        "    model = GTN(\n",
        "        num_features=X.shape[1],\n",
        "        num_classes=len(set(Y.cpu().numpy())),\n",
        "        hidden_feats=hidden_feats,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(train_data)\n",
        "        loss = criterion(out, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(test_data)\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
        "            scheduler.step(acc)\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(test_data)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        test_acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
        "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro', zero_division=1)\n",
        "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
        "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
        "\n",
        "        accuracy_scores.append(test_acc)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        F1Measure.append(f1)\n",
        "print('Training time: %s' % (now() - t))\n",
        "\n",
        "# Calculate the average metrics across all folds\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "average_precision = np.mean(precision_scores)\n",
        "average_recall = np.mean(recall_scores)\n",
        "average_f1 = np.mean(F1Measure)\n",
        "\n",
        "print(\"Average accuracy =\", average_accuracy)\n",
        "print(\"Accuracy std dev =\", np.std(accuracy_scores))\n",
        "print(\"Average precision =\", average_precision)\n",
        "print(\"Precision std dev =\", np.std(precision_scores))\n",
        "print(\"Average recall =\", average_recall)\n",
        "print(\"Recall std dev =\", np.std(recall_scores))\n",
        "print(\"Average F1 score =\", average_f1)\n",
        "print(\"F1 std dev =\", np.std(F1Measure))"
      ],
      "metadata": {
        "id": "OdXHhVhZdgCD",
        "outputId": "79bf1434-7a5f-475e-f2c5-8bcb99cd9ff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OdXHhVhZdgCD",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "--2025-10-12 08:09:55--  https://www.idahofallshighered.org/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n",
            "Resolving www.idahofallshighered.org (www.idahofallshighered.org)... 50.6.2.30\n",
            "Connecting to www.idahofallshighered.org (www.idahofallshighered.org)|50.6.2.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123599052 (118M) [text/csv]\n",
            "Saving to: ‘mRNA_miRNA_Meth_integrated.csv’\n",
            "\n",
            "mRNA_miRNA_Meth_int 100%[===================>] 117.87M  12.5MB/s    in 12s     \n",
            "\n",
            "2025-10-12 08:10:08 (10.0 MB/s) - ‘mRNA_miRNA_Meth_integrated.csv’ saved [123599052/123599052]\n",
            "\n",
            "Number of classes: 32\n",
            "Number of samples: 8464\n",
            "Number of Features: 2793\n",
            "Training time: 0:01:16.765321\n",
            "Average accuracy = 0.9519140837183844\n",
            "Accuracy std dev = 0.0029065328519527326\n",
            "Average precision = 0.9440743116642271\n",
            "Precision std dev = 0.0073944796810965436\n",
            "Average recall = 0.928847047941732\n",
            "Recall std dev = 0.011279022053954738\n",
            "Average F1 score = 0.9306938825294158\n",
            "F1 std dev = 0.011996182550984454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import TransformerConv\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import datetime\n",
        "\n",
        "now = datetime.datetime.now\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess the multi-omics data\n",
        "!wget https://www.idahofallshighered.org/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n",
        "file_path = 'mRNA_miRNA_Meth_integrated.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "Y = df.iloc[:, -1].copy()\n",
        "\n",
        "# Remove non-numeric columns\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "X = df.values\n",
        "\n",
        "num_classes = len(set(Y))\n",
        "print(\"Number of classes:\", num_classes)\n",
        "num_samples = X.shape[0]\n",
        "print(\"Number of samples:\", num_samples)\n",
        "num_features = X.shape[1]\n",
        "print(\"Number of features:\", num_features)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(Y)\n",
        "\n",
        "# Define the GTN model\n",
        "class GTN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(GTN, self).__init__()\n",
        "        self.conv1 = TransformerConv(num_features, 128, heads=8)  # Output: 128 * 8 = 1024\n",
        "        self.conv2 = TransformerConv(1024, num_classes, heads=1)  # Input: 1024, Output: num_classes * 1\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)  # Add activation for stability\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Set up K-fold cross-validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Initialize lists to store metrics for each fold\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "accuracy_scores = []\n",
        "F1Measure = []\n",
        "\n",
        "# Set hyperparameters\n",
        "lr = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Training and Evaluation\n",
        "t = now()\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    # Convert labels to tensors\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "    # Calculate the correlation matrix and convert it to an edge index\n",
        "    correlation_matrix_train = np.corrcoef(X_train, rowvar=True)\n",
        "    correlation_matrix_test = np.corrcoef(X_test, rowvar=True)\n",
        "\n",
        "    # Handle potential NaN values in correlation matrix\n",
        "    correlation_matrix_train = np.nan_to_num(correlation_matrix_train, nan=0.0)\n",
        "    correlation_matrix_test = np.nan_to_num(correlation_matrix_test, nan=0.0)\n",
        "\n",
        "    # Create edge indices based on a correlation threshold\n",
        "    edge_index_train = torch.tensor(np.argwhere((correlation_matrix_train >= 0.9) | (correlation_matrix_train <= -0.9)).T, dtype=torch.long).to(device)\n",
        "    edge_index_test = torch.tensor(np.argwhere((correlation_matrix_test >= 0.9) | (correlation_matrix_test <= -0.9)).T, dtype=torch.long).to(device)\n",
        "\n",
        "    # Prepare training and testing data\n",
        "    train_data = Data(x=torch.tensor(X_train, dtype=torch.float32).to(device), edge_index=edge_index_train, y=y_train)\n",
        "    test_data = Data(x=torch.tensor(X_test, dtype=torch.float32).to(device), edge_index=edge_index_test, y=y_test)\n",
        "\n",
        "    # Initialize the model, criterion, optimizer, and scheduler\n",
        "    model = GTN(X.shape[1], len(np.unique(labels))).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(train_data)\n",
        "        loss = criterion(out, train_data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(test_data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
        "            scheduler.step(acc)\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(test_data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
        "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro', zero_division=1)\n",
        "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro', zero_division=1)\n",
        "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
        "\n",
        "        accuracy_scores.append(acc)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        F1Measure.append(f1)\n",
        "print('Training time: %s' % (now() - t))\n",
        "\n",
        "# Calculate the average metrics across all folds\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "average_precision = np.mean(precision_scores)\n",
        "average_recall = np.mean(recall_scores)\n",
        "average_f1 = np.mean(F1Measure)\n",
        "\n",
        "print(\"Average accuracy =\", average_accuracy)\n",
        "print(\"Accuracy std dev =\", np.std(accuracy_scores))\n",
        "print(\"Average precision =\", average_precision)\n",
        "print(\"Precision std dev =\", np.std(precision_scores))\n",
        "print(\"Average recall =\", average_recall)\n",
        "print(\"Recall std dev =\", np.std(recall_scores))\n",
        "print(\"Average F1 score =\", average_f1)\n",
        "print(\"F1 std dev =\", np.std(F1Measure))"
      ],
      "metadata": {
        "id": "AluhHrutdgmt",
        "outputId": "34899903-aced-466d-b43c-18a18b95ff35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AluhHrutdgmt",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "--2025-10-12 08:15:21--  https://www.idahofallshighered.org/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n",
            "Resolving www.idahofallshighered.org (www.idahofallshighered.org)... 50.6.2.30\n",
            "Connecting to www.idahofallshighered.org (www.idahofallshighered.org)|50.6.2.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123599052 (118M) [text/csv]\n",
            "Saving to: ‘mRNA_miRNA_Meth_integrated.csv.1’\n",
            "\n",
            "mRNA_miRNA_Meth_int 100%[===================>] 117.87M  13.5MB/s    in 11s     \n",
            "\n",
            "2025-10-12 08:15:33 (10.5 MB/s) - ‘mRNA_miRNA_Meth_integrated.csv.1’ saved [123599052/123599052]\n",
            "\n",
            "Number of classes: 32\n",
            "Number of samples: 8464\n",
            "Number of features: 2793\n",
            "Training time: 0:01:18.287837\n",
            "Average accuracy = 0.9456525199716814\n",
            "Accuracy std dev = 0.00627047367484188\n",
            "Average precision = 0.9417184499972985\n",
            "Precision std dev = 0.010023656341551368\n",
            "Average recall = 0.9150668455389717\n",
            "Recall std dev = 0.010960405123904218\n",
            "Average F1 score = 0.9188873386680413\n",
            "F1 std dev = 0.008724934113374279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BR4rWw3fQ5N"
      },
      "id": "6BR4rWw3fQ5N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}