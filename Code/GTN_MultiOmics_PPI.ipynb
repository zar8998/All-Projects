{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zar8998/All-Projects/blob/main/Code/GTN_MultiOmics_PPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PlGRFpmfIymD",
      "metadata": {
        "id": "PlGRFpmfIymD"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/avakanski/Graph_NNs_for_Multi_Omics_Integration/blob/main/Code/GTN_MultiOmics_PPI.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36uvxgtPcRlS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36uvxgtPcRlS",
        "outputId": "6702d019-3f42-40d3-f7ec-1bdd6cacaa32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RKxbDYxUcUYy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKxbDYxUcUYy",
        "outputId": "0ff57fbf-5c59-49dd-8aa8-9efb06a2ba74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qMf4PWEciul",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qMf4PWEciul",
        "outputId": "73e69ac9-4412-4159-ef05-63bf8dc435f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# GTN using multi-omics data (mRNA, miRNA and DNA methylation) with PPI graph structure (5 fold cross validation)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import TransformerConv\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import datetime\n",
        "now = datetime.datetime.now\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oUA1r1fzco-L",
      "metadata": {
        "id": "oUA1r1fzco-L"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the PPI data\n",
        "ppi_file_path = 'drive/My Drive/Projects/Gene_Expression_Project/PPI.csv'\n",
        "ppi_df = pd.read_csv(ppi_file_path)\n",
        "\n",
        "# Step 2: Concatenate 'stringId_A' and 'stringId_B' to calculate the number of connections (degree)\n",
        "all_proteins = pd.concat([ppi_df['stringId_A'], ppi_df['stringId_B']])\n",
        "\n",
        "# Step 3: Count the number of connections for each protein\n",
        "protein_connections = all_proteins.value_counts()\n",
        "\n",
        "# Step 4: Define a degree threshold to select only highly connected proteins (e.g., 200 or more connections)\n",
        "degree_threshold = 200\n",
        "high_degree_proteins = protein_connections[protein_connections >= degree_threshold].index\n",
        "\n",
        "# Step 5: Filter the PPI data to include only edges where both proteins have a high number of connections\n",
        "ppi_filtered = ppi_df[\n",
        "    ppi_df['stringId_A'].isin(high_degree_proteins) &\n",
        "    ppi_df['stringId_B'].isin(high_degree_proteins)\n",
        "]\n",
        "\n",
        "# Step 6: Map the high-degree proteins to unique node IDs\n",
        "proteins = pd.concat([ppi_filtered['stringId_A'], ppi_filtered['stringId_B']]).unique()\n",
        "protein_to_id = {protein: idx for idx, protein in enumerate(proteins)}\n",
        "\n",
        "# Step 7: Create edge index (this will be the input for GTN)\n",
        "edges = ppi_filtered[['stringId_A', 'stringId_B']].map(lambda x: protein_to_id[x])\n",
        "edge_index = torch.tensor(edges.values.T, dtype=torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s-FXwJg3eY2Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-FXwJg3eY2Z",
        "outputId": "0aa44284-5dab-497e-e5e2-ed065c493bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-07 18:48:58--  https://www.webpages.uidaho.edu/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n",
            "Resolving www.webpages.uidaho.edu (www.webpages.uidaho.edu)... 129.101.105.230\n",
            "Connecting to www.webpages.uidaho.edu (www.webpages.uidaho.edu)|129.101.105.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123599052 (118M) [application/octet-stream]\n",
            "Saving to: ‘mRNA_miRNA_Meth_integrated.csv’\n",
            "\n",
            "mRNA_miRNA_Meth_int 100%[===================>] 117.87M  37.8MB/s    in 3.7s    \n",
            "\n",
            "2025-03-07 18:49:03 (31.7 MB/s) - ‘mRNA_miRNA_Meth_integrated.csv’ saved [123599052/123599052]\n",
            "\n",
            "Number of classes: 32\n",
            "Number of samples: 8464\n",
            "Number of Features: 2793\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Load and preprocess the multi-omics data\n",
        "!wget https://www.idahofallshighered.org/vakanski/Codes_Data/mRNA_miRNA_Meth_integrated.csv\n",
        "file_path = 'mRNA_miRNA_Meth_integrated.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "Y = df.iloc[:, -1].copy()\n",
        "\n",
        "# Remove non-numeric columns\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "X = df.values\n",
        "\n",
        "num_classes = len(set(Y))\n",
        "print(\"Number of classes:\", num_classes)\n",
        "num_samples = X.shape[0]\n",
        "print(\"Number of samples:\", num_samples)\n",
        "num_Features = X.shape[1]\n",
        "print(\"Number of Features:\", num_Features)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "\n",
        "# Convert data to PyTorch tensors and move to device\n",
        "X = torch.tensor(X, dtype=torch.float).to(device)\n",
        "Y = torch.tensor(Y, dtype=torch.long).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mIT3wokBenfm",
      "metadata": {
        "id": "mIT3wokBenfm"
      },
      "outputs": [],
      "source": [
        "# Step 9: Create PyTorch Geometric data object using the edge_index from the filtered PPI network\n",
        "data = Data(x=X, edge_index=edge_index)\n",
        "\n",
        "# Step 10: Define the GTN model\n",
        "class GTN(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, hidden_feats=1024, num_layers=2, dropout=0.5):\n",
        "        super(GTN, self).__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(TransformerConv(num_features, hidden_feats, heads=1, dropout=dropout))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(TransformerConv(hidden_feats, hidden_feats, heads=1, dropout=dropout))\n",
        "        self.fc = nn.Linear(hidden_feats, num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 11: Set up K-fold cross-validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Initialize lists to store metrics for each fold\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "accuracy_scores = []\n",
        "F1Measure = []\n",
        "\n",
        "# Set hyperparameters\n",
        "hidden_feats = 1024\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "lr = 0.001\n",
        "weight_decay = 0\n",
        "num_epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b18004c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b18004c",
        "outputId": "049a9def-1fc5-4eee-ccb7-d236b8965c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 0:01:12.276266\n"
          ]
        }
      ],
      "source": [
        "# Step 12: Training and Evaluation\n",
        "t = now()\n",
        "for train_index, test_index in kf.split(X.cpu()):\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    # Create train/test data using the same PPI edge_index\n",
        "    train_data = Data(x=X_train, edge_index=edge_index)\n",
        "    test_data = Data(x=X_test, edge_index=edge_index)\n",
        "\n",
        "    # Create the GTN model\n",
        "    model = GTN(\n",
        "        num_features=X.shape[1],\n",
        "        num_classes=len(set(Y.cpu().numpy())),\n",
        "        hidden_feats=hidden_feats,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(train_data)\n",
        "        loss = criterion(out, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(test_data)\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
        "            # print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n",
        "            scheduler.step(acc)\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(test_data)\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        test_acc = accuracy_score(y_test.cpu().numpy(), pred.cpu().numpy())\n",
        "        precision = precision_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro', zero_division=1)\n",
        "        recall = recall_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
        "        f1 = f1_score(y_test.cpu().numpy(), pred.cpu().numpy(), average='macro')\n",
        "\n",
        "        accuracy_scores.append(test_acc)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        F1Measure.append(f1)\n",
        "print('Training time: %s' % (now() - t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ISrYr9jHe80R",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISrYr9jHe80R",
        "outputId": "2ada736a-8220-45b1-e094-0e84d0fa4fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy = 0.9520330550354051\n",
            "Accuracy std sev = 0.004433865609198085\n",
            "Average precision = 0.9446092787639447\n",
            "Precision std sev = 0.007422989624697369\n",
            "Average recall = 0.9209231654365416\n",
            "Recall std sev = 0.013145008384676457\n",
            "Average F1 score = 0.9230865539887884\n",
            "F1 std dev = 0.012157202620385159\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average metrics across all folds\n",
        "average_accuracy = np.mean(accuracy_scores)\n",
        "average_precision = np.mean(precision_scores)\n",
        "average_recall = np.mean(recall_scores)\n",
        "average_f1 = np.mean(F1Measure)\n",
        "\n",
        "print(\"Average accuracy =\", average_accuracy)\n",
        "print(\"Accuracy std sev =\", np.std(accuracy_scores))\n",
        "print(\"Average precision =\", average_precision)\n",
        "print(\"Precision std sev =\", np.std(precision_scores))\n",
        "print(\"Average recall =\", average_recall)\n",
        "print(\"Recall std sev =\", np.std(recall_scores))\n",
        "print(\"Average F1 score =\", average_f1)\n",
        "print(\"F1 std dev =\", np.std(F1Measure))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN using multi-omics data (mRNA, miRNA, DNA methylation) with correlation matrix graph structure (5-fold cross-validation)\n",
        "# Optimized for Google Colab with manual file upload and CSV error handling\n",
        "\n",
        "# Install required libraries\n",
        "!pip install -q torch-geometric\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GraphConv\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prompt user to upload the dataset\n",
        "from google.colab import files\n",
        "print(\"Please upload 'mRNA_miRNA_Meth_integrated.csv'.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load CSV with error handling\n",
        "file_name = list(uploaded.keys())[0]\n",
        "try:\n",
        "    # Attempt to load with different delimiters and inspect the file\n",
        "    df = pd.read_csv(file_name, sep=',', engine='python', on_bad_lines='warn')\n",
        "    print(f\"Successfully loaded CSV. Shape: {df.shape}\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"ParserError: {e}\")\n",
        "    print(\"Trying with alternative delimiter (e.g., tab or semicolon)...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_name, sep='\\t', engine='python', on_bad_lines='warn')\n",
        "        print(f\"Loaded with tab delimiter. Shape: {df.shape}\")\n",
        "    except:\n",
        "        try:\n",
        "            df = pd.read_csv(file_name, sep=';', engine='python', on_bad_lines='warn')\n",
        "            print(f\"Loaded with semicolon delimiter. Shape: {df.shape}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to load CSV: {e2}\")\n",
        "            print(\"Please inspect the CSV file for issues (e.g., delimiter, missing values, or row length mismatches).\")\n",
        "            raise\n",
        "\n",
        "# Inspect the first few rows and columns\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "print(f\"\\nNumber of columns: {df.shape[1]}\")\n",
        "\n",
        "# Preprocess data\n",
        "try:\n",
        "    df.drop(df.columns[0], axis=1, inplace=True)  # Drop first column (assumed index)\n",
        "    Y = df.iloc[:, -1].copy()  # Extract labels\n",
        "except IndexError as e:\n",
        "    print(f\"Error accessing columns: {e}\")\n",
        "    print(\"Check if the CSV has at least 2 columns (features + label).\")\n",
        "    raise\n",
        "\n",
        "# Remove non-numeric columns\n",
        "df = df.select_dtypes(include=[np.number])\n",
        "X = df.values\n",
        "\n",
        "# Print dataset details\n",
        "num_classes = len(set(Y))\n",
        "num_samples = X.shape[0]\n",
        "num_features = X.shape[1]\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Number of samples: {num_samples}\")\n",
        "print(f\"Number of features: {num_features}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "\n",
        "# Define the GCN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, hidden_feats, num_classes, num_layers=3, dropout=0.3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv_layers = nn.ModuleList([GraphConv(in_feats, hidden_feats)])\n",
        "        self.conv_layers.extend([GraphConv(hidden_feats, hidden_feats) for _ in range(num_layers - 1)])\n",
        "        self.final_conv = GraphConv(hidden_feats, num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for conv in self.conv_layers:\n",
        "            x = torch.relu(conv(x, edge_index))\n",
        "            x = self.dropout(x)\n",
        "        x = self.final_conv(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Set up 5-fold cross-validation\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize lists to store metrics\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "accuracy_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "# Training and evaluation\n",
        "start_time = datetime.datetime.now()\n",
        "print(f\"Training started at: {start_time}\")\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
        "    print(f\"\\nFold {fold}/{k}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train = torch.FloatTensor(X_train).to(device)\n",
        "    y_train = torch.LongTensor(y_train).to(device)\n",
        "    X_test = torch.FloatTensor(X_test).to(device)\n",
        "    y_test = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "    # Calculate correlation matrices and create edge indices\n",
        "    correlation_matrix_train = np.corrcoef(X_train.cpu().numpy(), rowvar=False)\n",
        "    correlation_matrix_test = np.corrcoef(X_test.cpu().numpy(), rowvar=False)\n",
        "\n",
        "    # Handle potential NaN values\n",
        "    correlation_matrix_train = np.nan_to_num(correlation_matrix_train, nan=0.0)\n",
        "    correlation_matrix_test = np.nan_to_num(correlation_matrix_test, nan=0.0)\n",
        "\n",
        "    # Create edge indices based on correlation threshold (0.9)\n",
        "    src_train, dst_train = np.where(correlation_matrix_train > 0.9)\n",
        "    src_test, dst_test = np.where(correlation_matrix_test > 0.9)\n",
        "\n",
        "    edge_index_train = torch.tensor([src_train, dst_train], dtype=torch.long).to(device)\n",
        "    edge_index_test = torch.tensor([src_test, dst_test], dtype=torch.long).to(device)\n",
        "\n",
        "    # Create PyTorch Geometric data objects\n",
        "    train_data = Data(x=X_train, edge_index=edge_index_train).to(device)\n",
        "    test_data = Data(x=X_test, edge_index=edge_index_test).to(device)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = GCN(in_feats=num_features, hidden_feats=512, num_classes=num_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 100\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        outputs = model(train_data)\n",
        "        loss = criterion(outputs, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress every 20 epochs\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                y_pred = torch.argmax(model(test_data), dim=1)\n",
        "                acc = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n",
        "                print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch.argmax(model(test_data), dim=1)\n",
        "        test_acc = accuracy_score(y_test.cpu().numpy(), y_pred.cpu().numpy())\n",
        "        precision = precision_score(y_test.cpu().numpy(), y_pred.cpu().numpy(), average='macro', zero_division=1)\n",
        "        recall = recall_score(y_test.cpu().numpy(), y_pred.cpu().numpy(), average='macro')\n",
        "        f1 = f1_score(y_test.cpu().numpy(), y_pred.cpu().numpy(), average='macro')\n",
        "\n",
        "        accuracy_scores.append(test_acc)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold {fold} - Accuracy: {test_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Print final results\n",
        "print(f\"\\nTraining completed. Total time: {datetime.datetime.now() - start_time}\")\n",
        "print(f\"Average accuracy: {np.mean(accuracy_scores):.4f} ± {np.std(accuracy_scores):.4f}\")\n",
        "print(f\"Average precision: {np.mean(precision_scores):.4f} ± {np.std(precision_scores):.4f}\")\n",
        "print(f\"Average recall: {np.mean(recall_scores):.4f} ± {np.std(recall_scores):.4f}\")\n",
        "print(f\"Average F1 score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")"
      ],
      "metadata": {
        "id": "O_-ii4_uDgZ0"
      },
      "id": "O_-ii4_uDgZ0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}